# LinkU æ•°æ®åº“ä¼˜åŒ–æ•ˆæœæ£€æŸ¥æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›è¯¦ç»†çš„æ£€æŸ¥æ–¹æ³•ï¼Œç”¨äºéªŒè¯æ•°æ®åº“ä¼˜åŒ–æ˜¯å¦æˆåŠŸå®æ–½ã€‚

**æ£€æŸ¥åŸåˆ™**:
- âœ… ä½¿ç”¨æ•°æ®è¯´è¯ï¼ˆé‡åŒ–æŒ‡æ ‡ï¼‰
- âœ… å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½
- âœ… éªŒè¯ç´¢å¼•æ˜¯å¦çœŸæ­£è¢«ä½¿ç”¨
- âœ… ç¡®è®¤æ²¡æœ‰å¼•å…¥æ–°çš„é—®é¢˜

---

## ğŸ” æ£€æŸ¥æ–¹æ³•æ€»è§ˆ

### å¿«é€Ÿæ£€æŸ¥æ¸…å•

- [ ] **æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”**ï¼šå“åº”æ—¶é—´ã€QPSã€æ•°æ®åº“è´Ÿè½½
- [ ] **N+1 æŸ¥è¯¢æ£€æµ‹**ï¼šç¡®è®¤æŸ¥è¯¢æ¬¡æ•°åˆç†
- [ ] **ç´¢å¼•ä½¿ç”¨éªŒè¯**ï¼šä½¿ç”¨ EXPLAIN ANALYZE ç¡®è®¤ç´¢å¼•ç”Ÿæ•ˆ
- [ ] **ç¼“å­˜æ•ˆæœæ£€æŸ¥**ï¼šç¼“å­˜å‘½ä¸­ç‡ã€ç¼“å­˜å¤§å°
- [ ] **åŠŸèƒ½æ­£ç¡®æ€§**ï¼šæ•°æ®å‡†ç¡®æ€§ã€åˆ†é¡µæ­£ç¡®æ€§

---

## 1. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”

### 1.1 å“åº”æ—¶é—´å¯¹æ¯”

**æ£€æŸ¥æ–¹æ³•**:

#### æ–¹æ³•Aï¼šä½¿ç”¨åº”ç”¨æ—¥å¿—ï¼ˆæ¨èï¼‰

åœ¨å…³é”®æ¥å£æ·»åŠ æ€§èƒ½æ—¥å¿—ï¼š

```python
# backend/app/async_routers.py
import time
import logging

logger = logging.getLogger(__name__)

@async_router.get("/tasks")
async def get_tasks(...):
    start_time = time.time()
    try:
        # ... æŸ¥è¯¢é€»è¾‘ ...
        result = await async_crud.async_task_crud.get_tasks_with_total(...)
        elapsed = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        logger.info(
            f"GET /tasks - elapsed: {elapsed:.2f}ms, "
            f"task_count: {len(result[0])}, "
            f"total: {result[1]}"
        )
        
        return result
    except Exception as e:
        elapsed = (time.time() - start_time) * 1000
        logger.error(f"GET /tasks - error after {elapsed:.2f}ms: {e}")
        raise
```

**æ£€æŸ¥æ­¥éª¤**:

1. **æ”¶é›†ä¼˜åŒ–å‰çš„æ•°æ®**ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼Œå¯ä»¥ä¸´æ—¶æ·»åŠ æ—¥å¿—è¿è¡Œä¸€æ®µæ—¶é—´ï¼‰
   ```bash
   # æŸ¥çœ‹æ—¥å¿—ä¸­çš„å“åº”æ—¶é—´
   grep "GET /tasks" app.log | awk '{print $NF}' | sort -n
   ```

2. **æ”¶é›†ä¼˜åŒ–åçš„æ•°æ®**
   ```bash
   # åŒæ ·çš„å‘½ä»¤
   grep "GET /tasks" app.log | awk '{print $NF}' | sort -n
   ```

3. **å¯¹æ¯”åˆ†æ**
   ```python
   # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
   # ä¼˜åŒ–å‰ï¼šå¹³å‡ 300msï¼ŒP95: 500msï¼ŒP99: 800ms
   # ä¼˜åŒ–åï¼šå¹³å‡ 80msï¼ŒP95: 150msï¼ŒP99: 250ms
   # æå‡ï¼šå¹³å‡ 3.75å€ï¼ŒP95: 3.33å€ï¼ŒP99: 3.2å€
   ```

#### æ–¹æ³•Bï¼šä½¿ç”¨ APM å·¥å…·ï¼ˆå¦‚ New Relicã€Datadogï¼‰

å¦‚æœé¡¹ç›®å·²é›†æˆ APM å·¥å…·ï¼Œå¯ä»¥ç›´æ¥æŸ¥çœ‹ï¼š
- å¹³å‡å“åº”æ—¶é—´
- P95/P99 å“åº”æ—¶é—´
- è¯·æ±‚é‡ï¼ˆQPSï¼‰
- é”™è¯¯ç‡

#### æ–¹æ³•Cï¼šä½¿ç”¨å‹æµ‹å·¥å…·ï¼ˆæ¨èç”¨äºéªŒè¯ï¼‰

ä½¿ç”¨ **Locust** æˆ– **k6** è¿›è¡Œå‹æµ‹ï¼š

```python
# locustfile.py
from locust import HttpUser, task, between
import time

class TaskListUser(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def get_tasks(self):
        start = time.time()
        response = self.client.get("/tasks?limit=20&sort_by=latest")
        elapsed = (time.time() - start) * 1000
        
        # è®°å½•å“åº”æ—¶é—´
        if response.status_code == 200:
            print(f"Response time: {elapsed:.2f}ms")
        else:
            print(f"Error: {response.status_code}, time: {elapsed:.2f}ms")
```

**è¿è¡Œå‹æµ‹**:
```bash
# å¯åŠ¨ Locust
locust -f locustfile.py --host=http://localhost:8000

# è®¿é—® http://localhost:8089 è¿›è¡Œå‹æµ‹
# è®¾ç½®ï¼š100 å¹¶å‘ç”¨æˆ·ï¼ŒæŒç»­ 5 åˆ†é’Ÿ
```

**æ£€æŸ¥æŒ‡æ ‡**:
- å¹³å‡å“åº”æ—¶é—´ï¼ˆåº”è¯¥ < 100msï¼‰
- P95 å“åº”æ—¶é—´ï¼ˆåº”è¯¥ < 200msï¼‰
- å¤±è´¥ç‡ï¼ˆåº”è¯¥ < 0.1%ï¼‰
- RPSï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼Œåº”è¯¥ > 50ï¼‰

### 1.2 æ•°æ®åº“è´Ÿè½½å¯¹æ¯”

**æ£€æŸ¥æ–¹æ³•**:

#### æŸ¥çœ‹ PostgreSQL ç»Ÿè®¡ä¿¡æ¯

```sql
-- 1. æŸ¥çœ‹æŸ¥è¯¢æ‰§è¡Œæ—¶é—´ç»Ÿè®¡ï¼ˆéœ€è¦å¯ç”¨ pg_stat_statementsï¼‰
SELECT 
    query,
    calls,
    total_exec_time::numeric / 1000 as total_seconds,
    mean_exec_time::numeric / 1000 as mean_seconds,
    max_exec_time::numeric / 1000 as max_seconds
FROM pg_stat_statements
WHERE query LIKE '%tasks%'
ORDER BY total_exec_time DESC
LIMIT 10;

-- 2. æŸ¥çœ‹æ•°æ®åº“è¿æ¥æ•°
SELECT count(*) FROM pg_stat_activity;

-- 3. æŸ¥çœ‹æ…¢æŸ¥è¯¢ï¼ˆæ‰§è¡Œæ—¶é—´ > 100msï¼‰
SELECT 
    query,
    calls,
    mean_exec_time::numeric / 1000 as mean_seconds
FROM pg_stat_statements
WHERE mean_exec_time > 100000  -- 100ms in microseconds
ORDER BY mean_exec_time DESC
LIMIT 20;
```

**ä¼˜åŒ–å‰åå¯¹æ¯”**:
- æŸ¥è¯¢æ‰§è¡Œæ—¶é—´åº”è¯¥é™ä½ 50-80%
- æ…¢æŸ¥è¯¢æ•°é‡åº”è¯¥æ˜¾è‘—å‡å°‘
- æ•°æ®åº“è¿æ¥æ•°åº”è¯¥ä¿æŒç¨³å®š

---

## 2. N+1 æŸ¥è¯¢æ£€æµ‹

### 2.1 ä½¿ç”¨ SQLAlchemy æ—¥å¿—

**å¯ç”¨ SQL æ—¥å¿—**:

```python
# backend/app/database.py
import logging

# å¯ç”¨ SQLAlchemy æ—¥å¿—
logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)

# æˆ–è€…åœ¨åˆ›å»ºå¼•æ“æ—¶è®¾ç½®
engine = create_async_engine(
    DATABASE_URL,
    echo=True,  # æ‰“å°æ‰€æœ‰ SQL æŸ¥è¯¢
    echo_pool=True  # æ‰“å°è¿æ¥æ± ä¿¡æ¯
)
```

**æ£€æŸ¥æ–¹æ³•**:

1. **è§¦å‘ä¸€ä¸ªè¯·æ±‚**ï¼ˆä¾‹å¦‚ï¼šè·å–ç”¨æˆ·ç”³è¯·åˆ—è¡¨ï¼‰
   ```bash
   curl -X GET "http://localhost:8000/my-applications" \
     -H "Cookie: session_id=xxx"
   ```

2. **æŸ¥çœ‹æ—¥å¿—ä¸­çš„ SQL æŸ¥è¯¢**
   ```bash
   # åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„æ—¥å¿—
   # ä¼˜åŒ–å‰ï¼ˆN+1 æŸ¥è¯¢ï¼‰:
   # SELECT * FROM task_applications WHERE applicant_id = '123'  -- 1 æ¬¡
   # SELECT * FROM tasks WHERE id = 1  -- N æ¬¡ï¼ˆæ¯ä¸ªç”³è¯·ä¸€æ¬¡ï¼‰
   # SELECT * FROM tasks WHERE id = 2  -- N æ¬¡
   # ... (æ€»å…± N+1 æ¬¡æŸ¥è¯¢)
   
   # ä¼˜åŒ–åï¼ˆæ‰¹é‡æŸ¥è¯¢ï¼‰:
   # SELECT * FROM task_applications WHERE applicant_id = '123'  -- 1 æ¬¡
   # SELECT * FROM tasks WHERE id IN (1, 2, 3, ...)  -- 1 æ¬¡ï¼ˆæ‰¹é‡æŸ¥è¯¢ï¼‰
   # (æ€»å…± 2 æ¬¡æŸ¥è¯¢)
   ```

3. **ç»Ÿè®¡æŸ¥è¯¢æ¬¡æ•°**
   ```python
   # ä½¿ç”¨ SQLAlchemy äº‹ä»¶ç›‘å¬å™¨
   from sqlalchemy import event
   from sqlalchemy.engine import Engine
   
   query_count = 0
   
   @event.listens_for(Engine, "before_cursor_execute")
   def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
       global query_count
       query_count += 1
       print(f"Query #{query_count}: {statement[:100]}...")
   
   # åœ¨è¯·æ±‚å¤„ç†å‡½æ•°ä¸­é‡ç½®è®¡æ•°å™¨
   @async_router.get("/my-applications")
   async def get_user_applications(...):
       global query_count
       query_count = 0
       
       # ... æŸ¥è¯¢é€»è¾‘ ...
       
       print(f"Total queries: {query_count}")
       return result
   ```

### 2.2 ä½¿ç”¨ Django Debug Toolbar é£æ ¼çš„å·¥å…·

**åˆ›å»ºæŸ¥è¯¢ç›‘æ§ä¸­é—´ä»¶**:

```python
# backend/app/middleware/query_monitor.py
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware
import time
import logging

logger = logging.getLogger(__name__)

class QueryMonitorMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        from sqlalchemy import event
        from sqlalchemy.engine import Engine
        
        query_count = 0
        query_times = []
        
        def count_queries(conn, cursor, statement, parameters, context, executemany):
            nonlocal query_count
            query_count += 1
            start = time.time()
            return start
        
        def log_query_time(conn, cursor, statement, parameters, context, executemany, start):
            elapsed = (time.time() - start) * 1000
            query_times.append(elapsed)
            if elapsed > 100:  # æ…¢æŸ¥è¯¢è­¦å‘Š
                logger.warning(f"Slow query ({elapsed:.2f}ms): {statement[:200]}")
        
        # æ³¨å†Œäº‹ä»¶ç›‘å¬å™¨
        event.listen(Engine, "before_cursor_execute", count_queries)
        event.listen(Engine, "after_cursor_execute", log_query_time)
        
        try:
            response = await call_next(request)
            
            # åœ¨å“åº”å¤´ä¸­æ·»åŠ æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
            response.headers["X-Query-Count"] = str(query_count)
            response.headers["X-Total-Query-Time"] = f"{sum(query_times):.2f}ms"
            
            # è®°å½•åˆ°æ—¥å¿—
            if query_count > 10:  # å¦‚æœæŸ¥è¯¢æ¬¡æ•°è¿‡å¤šï¼Œè®°å½•è­¦å‘Š
                logger.warning(
                    f"{request.url.path} - {query_count} queries, "
                    f"total time: {sum(query_times):.2f}ms"
                )
            
            return response
        finally:
            # ç§»é™¤äº‹ä»¶ç›‘å¬å™¨
            event.remove(Engine, "before_cursor_execute", count_queries)
            event.remove(Engine, "after_cursor_execute", log_query_time)
```

**ä½¿ç”¨ä¸­é—´ä»¶**:

```python
# backend/main.py
from app.middleware.query_monitor import QueryMonitorMiddleware

app.add_middleware(QueryMonitorMiddleware)
```

**æ£€æŸ¥å“åº”å¤´**:

```bash
# æŸ¥çœ‹å“åº”å¤´ä¸­çš„æŸ¥è¯¢ç»Ÿè®¡
curl -I "http://localhost:8000/my-applications" \
  -H "Cookie: session_id=xxx"

# åº”è¯¥çœ‹åˆ°ï¼š
# X-Query-Count: 2  # ä¼˜åŒ–ååº”è¯¥æ˜¯ 2-5 æ¬¡
# X-Total-Query-Time: 45.23ms
```

**ä¼˜åŒ–ç›®æ ‡**:
- ç”¨æˆ·ç”³è¯·åˆ—è¡¨ï¼šä» N+1 æ¬¡æŸ¥è¯¢å‡å°‘åˆ° 2-3 æ¬¡
- ä»»åŠ¡ç”³è¯·åˆ—è¡¨ï¼šä» N+1 æ¬¡æŸ¥è¯¢å‡å°‘åˆ° 2-3 æ¬¡
- ä»»åŠ¡åˆ—è¡¨ï¼šä¿æŒ 1-2 æ¬¡æŸ¥è¯¢

---

## 3. ç´¢å¼•ä½¿ç”¨éªŒè¯

### 3.1 ä½¿ç”¨ EXPLAIN ANALYZE

**è¿™æ˜¯æœ€é‡è¦çš„æ£€æŸ¥æ–¹æ³•ï¼**

#### æ£€æŸ¥ä»»åŠ¡åˆ—è¡¨æŸ¥è¯¢

```sql
-- 1. åŸºç¡€æŸ¥è¯¢ï¼ˆåº”è¯¥ä½¿ç”¨ ix_tasks_status_created_idï¼‰
EXPLAIN ANALYZE
SELECT * FROM tasks 
WHERE status = 'open' 
ORDER BY created_at DESC, id DESC
LIMIT 20;

-- é¢„æœŸç»“æœï¼š
-- Limit  (cost=0.43..15.23 rows=20 width=xxx) (actual time=0.123..0.456 rows=20 loops=1)
--   ->  Index Scan using ix_tasks_status_created_id on tasks  (cost=0.43..xxx rows=xxx) (actual time=0.120..0.450 rows=20 loops=1)
--         Index Cond: (status = 'open'::text)
-- Planning Time: 0.123 ms
-- Execution Time: 0.567 ms
```

**å…³é”®æ£€æŸ¥ç‚¹**:
- âœ… åº”è¯¥çœ‹åˆ° `Index Scan using ix_tasks_status_created_id`
- âŒ ä¸åº”è¯¥çœ‹åˆ° `Seq Scan`ï¼ˆå…¨è¡¨æ‰«æï¼‰
- âœ… `Execution Time` åº”è¯¥ < 10ms

#### æ£€æŸ¥ä»»åŠ¡ç±»å‹+åœ°ç‚¹ç­›é€‰

```sql
-- 2. ç»„åˆç­›é€‰æŸ¥è¯¢ï¼ˆåº”è¯¥ä½¿ç”¨ ix_tasks_type_location_statusï¼‰
EXPLAIN ANALYZE
SELECT * FROM tasks 
WHERE status = 'open' 
  AND task_type = 'delivery'
  AND location = 'London'
ORDER BY created_at DESC
LIMIT 20;

-- é¢„æœŸç»“æœï¼š
-- Limit  (cost=0.43..15.23 rows=20 width=xxx) (actual time=0.123..0.456 rows=20 loops=1)
--   ->  Index Scan using ix_tasks_type_location_status on tasks  (cost=0.43..xxx rows=xxx) (actual time=0.120..0.450 rows=20 loops=1)
--         Index Cond: ((task_type = 'delivery'::text) AND (location = 'London'::text) AND (status = 'open'::text))
```

#### æ£€æŸ¥ç”¨æˆ·ä»»åŠ¡æŸ¥è¯¢

```sql
-- 3. ç”¨æˆ·å‘å¸ƒçš„ä»»åŠ¡ï¼ˆåº”è¯¥ä½¿ç”¨ ix_tasks_poster_status_createdï¼‰
EXPLAIN ANALYZE
SELECT * FROM tasks 
WHERE poster_id = '12345678'
  AND status = 'open'
ORDER BY created_at DESC
LIMIT 20;

-- é¢„æœŸç»“æœï¼š
-- Limit  (cost=0.43..15.23 rows=20 width=xxx) (actual time=0.123..0.456 rows=20 loops=1)
--   ->  Index Scan using ix_tasks_poster_status_created on tasks  (cost=0.43..xxx rows=xxx) (actual time=0.120..0.450 rows=20 loops=1)
--         Index Cond: ((poster_id = '12345678'::text) AND (status = 'open'::text))
```

#### æ£€æŸ¥å…³é”®è¯æœç´¢

```sql
-- 4. pg_trgm æœç´¢ï¼ˆåº”è¯¥ä½¿ç”¨ idx_tasks_title_trgmï¼‰
EXPLAIN ANALYZE
SELECT * FROM tasks 
WHERE status = 'open'
  AND similarity(title, 'delivery') > 0.2
ORDER BY similarity(title, 'delivery') DESC, created_at DESC
LIMIT 20;

-- é¢„æœŸç»“æœï¼š
-- Limit  (cost=0.43..15.23 rows=20 width=xxx) (actual time=0.123..0.456 rows=20 loops=1)
--   ->  Bitmap Index Scan using idx_tasks_title_trgm on tasks  (cost=0.43..xxx rows=xxx) (actual time=0.120..0.450 rows=20 loops=1)
--         Index Cond: (similarity((title)::text, 'delivery'::text) > '0.2'::double precision)
```

#### æ£€æŸ¥å¯¹è¯æ¶ˆæ¯æŸ¥è¯¢

```sql
-- 5. å¯¹è¯æ¶ˆæ¯æŸ¥è¯¢ï¼ˆåº”è¯¥ä½¿ç”¨ ix_messages_conversation_createdï¼‰
EXPLAIN ANALYZE
SELECT * FROM messages 
WHERE conversation_key = '12345678-87654321'
ORDER BY created_at ASC
LIMIT 50;

-- é¢„æœŸç»“æœï¼š
-- Limit  (cost=0.43..15.23 rows=50 width=xxx) (actual time=0.123..0.456 rows=50 loops=1)
--   ->  Index Scan using ix_messages_conversation_created on messages  (cost=0.43..xxx rows=xxx) (actual time=0.120..0.450 rows=50 loops=1)
--         Index Cond: (conversation_key = '12345678-87654321'::text)
```

### 3.2 æŸ¥çœ‹ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡

```sql
-- æŸ¥çœ‹æ‰€æœ‰ç´¢å¼•çš„ä½¿ç”¨æƒ…å†µ
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,  -- ç´¢å¼•æ‰«ææ¬¡æ•°
    idx_tup_read as tuples_read,  -- è¯»å–çš„å…ƒç»„æ•°
    idx_tup_fetch as tuples_fetched  -- è·å–çš„å…ƒç»„æ•°
FROM pg_stat_user_indexes
WHERE tablename IN ('tasks', 'task_applications', 'messages', 'notifications')
ORDER BY idx_scan DESC;

-- æ£€æŸ¥ç‚¹ï¼š
-- 1. æ–°åˆ›å»ºçš„ç´¢å¼•åº”è¯¥æœ‰ idx_scan > 0ï¼ˆè¡¨ç¤ºè¢«ä½¿ç”¨è¿‡ï¼‰
-- 2. å¦‚æœ idx_scan = 0ï¼Œè¯´æ˜ç´¢å¼•å¯èƒ½æ²¡æœ‰è¢«ä½¿ç”¨ï¼Œéœ€è¦æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶
```

### 3.3 æ£€æŸ¥æœªä½¿ç”¨çš„ç´¢å¼•

```sql
-- æŸ¥æ‰¾å¯èƒ½æœªä½¿ç”¨çš„ç´¢å¼•ï¼ˆidx_scan = 0 ä¸”åˆ›å»ºæ—¶é—´ > 1 å¤©ï¼‰
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND schemaname = 'public'
  AND tablename IN ('tasks', 'task_applications', 'messages', 'notifications')
ORDER BY pg_relation_size(indexrelid) DESC;

-- æ³¨æ„ï¼šæ–°åˆ›å»ºçš„ç´¢å¼•å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´æ‰ä¼šè¢«ä½¿ç”¨
-- å¦‚æœç´¢å¼•åˆ›å»ºå 1 å‘¨å†…éƒ½æ²¡æœ‰è¢«ä½¿ç”¨ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ï¼š
-- 1. æŸ¥è¯¢æ¡ä»¶æ˜¯å¦ä¸ç´¢å¼•åŒ¹é…
-- 2. ç´¢å¼•åˆ—é¡ºåºæ˜¯å¦æ­£ç¡®
-- 3. æ˜¯å¦éœ€è¦è°ƒæ•´æŸ¥è¯¢è¯­å¥
```

---

## 4. ç¼“å­˜æ•ˆæœæ£€æŸ¥

### 4.1 æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡

**æ·»åŠ ç¼“å­˜ç»Ÿè®¡**:

```python
# backend/app/redis_cache.py
import redis
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class CacheStats:
    def __init__(self):
        self.hits = 0
        self.misses = 0
    
    @property
    def hit_rate(self):
        total = self.hits + self.misses
        return (self.hits / total * 100) if total > 0 else 0

# å…¨å±€ç»Ÿè®¡å¯¹è±¡
cache_stats = CacheStats()

def get_tasks_list_with_stats(cache_params):
    """è·å–ä»»åŠ¡åˆ—è¡¨ï¼ˆå¸¦ç»Ÿè®¡ï¼‰"""
    cache_key = get_tasks_cache_key(**cache_params)
    
    cached = redis_client.get(cache_key)
    if cached:
        cache_stats.hits += 1
        return json.loads(cached)
    else:
        cache_stats.misses += 1
        return None

def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "hits": cache_stats.hits,
        "misses": cache_stats.misses,
        "hit_rate": f"{cache_stats.hit_rate:.2f}%",
        "total_requests": cache_stats.hits + cache_stats.misses
    }
```

**æ·»åŠ ç»Ÿè®¡æ¥å£**:

```python
# backend/app/async_routers.py
@async_router.get("/admin/cache-stats")
async def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…ç®¡ç†å‘˜ï¼‰"""
    from app.redis_cache import get_cache_stats
    return get_cache_stats()
```

**æ£€æŸ¥æ–¹æ³•**:

```bash
# 1. è§¦å‘ä¸€äº›è¯·æ±‚ï¼ˆæ¨¡æ‹Ÿæ­£å¸¸ä½¿ç”¨ï¼‰
for i in {1..100}; do
  curl "http://localhost:8000/tasks?limit=20&sort_by=latest" > /dev/null
done

# 2. æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
curl "http://localhost:8000/admin/cache-stats"

# é¢„æœŸç»“æœï¼š
# {
#   "hits": 85,
#   "misses": 15,
#   "hit_rate": "85.00%",
#   "total_requests": 100
# }
```

**ä¼˜åŒ–ç›®æ ‡**:
- ç¼“å­˜å‘½ä¸­ç‡åº”è¯¥ > 70%
- å¦‚æœå‘½ä¸­ç‡ < 50%ï¼Œéœ€è¦æ£€æŸ¥ï¼š
  - ç¼“å­˜é”®è®¾è®¡æ˜¯å¦åˆç†
  - ç¼“å­˜ TTL æ˜¯å¦å¤ªçŸ­
  - æ˜¯å¦æœ‰é¢‘ç¹çš„ç¼“å­˜å¤±æ•ˆ

### 4.2 æ£€æŸ¥ Redis å†…å­˜ä½¿ç”¨

```bash
# è¿æ¥åˆ° Redis
redis-cli

# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
INFO memory

# æŸ¥çœ‹ key æ•°é‡
DBSIZE

# æŸ¥çœ‹ç‰¹å®šæ¨¡å¼çš„ key æ•°é‡
KEYS "tasks:list:*" | wc -l
KEYS "tasks:count:*" | wc -l

# æŸ¥çœ‹ key å¤§å°
MEMORY USAGE "tasks:list:v1:abc123"
```

**ä¼˜åŒ–ç›®æ ‡**:
- Redis å†…å­˜ä½¿ç”¨åº”è¯¥ < 100MBï¼ˆå–å†³äºæ•°æ®é‡ï¼‰
- å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œéœ€è¦æ£€æŸ¥ï¼š
  - æ˜¯å¦æœ‰ç¼“å­˜æ³„æ¼ï¼ˆkey æ²¡æœ‰è¿‡æœŸï¼‰
  - ç¼“å­˜æ•°æ®æ˜¯å¦è¿‡å¤§
  - æ˜¯å¦éœ€è¦è°ƒæ•´ TTL

### 4.3 æ£€æŸ¥ç¼“å­˜å¤±æ•ˆ

```python
# æµ‹è¯•ç¼“å­˜å¤±æ•ˆé€»è¾‘
# 1. åˆ›å»ºä¸€ä¸ªä»»åŠ¡
POST /tasks
# 2. ç«‹å³æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨ï¼ˆåº”è¯¥çœ‹åˆ°æ–°ä»»åŠ¡ï¼‰
GET /tasks
# 3. æ£€æŸ¥ç¼“å­˜æ˜¯å¦å·²å¤±æ•ˆ
# 4. å†æ¬¡æŸ¥è¯¢ï¼ˆåº”è¯¥ä»æ•°æ®åº“é‡æ–°åŠ è½½ï¼‰
GET /tasks
```

---

## 5. åŠŸèƒ½æ­£ç¡®æ€§æ£€æŸ¥

### 5.1 åˆ†é¡µæ­£ç¡®æ€§

**æ£€æŸ¥æ¸¸æ ‡åˆ†é¡µ**:

```python
# æµ‹è¯•è„šæœ¬
import requests

base_url = "http://localhost:8000"
session_id = "your_session_id"

# 1. è·å–ç¬¬ä¸€é¡µ
response1 = requests.get(
    f"{base_url}/tasks",
    params={"limit": 20, "sort_by": "latest", "cursor": None},
    headers={"Cookie": f"session_id={session_id}"}
)
data1 = response1.json()
tasks1 = data1["items"]
next_cursor = data1.get("next_cursor")

# 2. è·å–ç¬¬äºŒé¡µ
response2 = requests.get(
    f"{base_url}/tasks",
    params={"limit": 20, "sort_by": "latest", "cursor": next_cursor},
    headers={"Cookie": f"session_id={session_id}"}
)
data2 = response2.json()
tasks2 = data2["items"]

# 3. æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
task_ids1 = {task["id"] for task in tasks1}
task_ids2 = {task["id"] for task in tasks2}
duplicates = task_ids1 & task_ids2

if duplicates:
    print(f"âŒ å‘ç°é‡å¤ä»»åŠ¡: {duplicates}")
else:
    print("âœ… æ²¡æœ‰é‡å¤ä»»åŠ¡")

# 4. æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼ï¼ˆé€šè¿‡æ€»æ•°éªŒè¯ï¼‰
total = data1.get("total") or (len(tasks1) + len(tasks2))
if total and len(task_ids1 | task_ids2) > total:
    print(f"âŒ ä»»åŠ¡æ•°é‡å¼‚å¸¸: æœŸæœ›æœ€å¤š {total}ï¼Œå®é™… {len(task_ids1 | task_ids2)}")
else:
    print("âœ… ä»»åŠ¡æ•°é‡æ­£å¸¸")
```

### 5.2 æ•°æ®ä¸€è‡´æ€§

**æ£€æŸ¥åˆ—è¡¨å’Œæ€»æ•°æ˜¯å¦ä¸€è‡´**:

```python
# æµ‹è¯•è„šæœ¬
import requests

def test_tasks_consistency():
    """æµ‹è¯•ä»»åŠ¡åˆ—è¡¨å’Œæ€»æ•°çš„ä¸€è‡´æ€§"""
    base_url = "http://localhost:8000"
    
    # è·å–ä»»åŠ¡åˆ—è¡¨å’Œæ€»æ•°
    response = requests.get(
        f"{base_url}/tasks",
        params={"limit": 100, "sort_by": "latest"}
    )
    data = response.json()
    
    tasks = data.get("tasks", [])
    total = data.get("total", 0)
    
    # æ£€æŸ¥
    if len(tasks) > total:
        print(f"âŒ åˆ—è¡¨æ•°é‡ ({len(tasks)}) å¤§äºæ€»æ•° ({total})")
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
    task_ids = [task["id"] for task in tasks]
    if len(task_ids) != len(set(task_ids)):
        print(f"âŒ å‘ç°é‡å¤ä»»åŠ¡ ID")
        return False
    
    print(f"âœ… æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡: {len(tasks)}/{total}")
    return True
```

---

## 6. è‡ªåŠ¨åŒ–æ£€æŸ¥è„šæœ¬

### 6.1 å®Œæ•´çš„æ£€æŸ¥è„šæœ¬

```python
# scripts/check_optimization.py
#!/usr/bin/env python3
"""
ä¼˜åŒ–æ•ˆæœæ£€æŸ¥è„šæœ¬
"""
import asyncio
import time
import requests
from typing import Dict, List
import json

class OptimizationChecker:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = {}
    
    def check_response_time(self, endpoint: str, params: Dict = None) -> Dict:
        """æ£€æŸ¥å“åº”æ—¶é—´"""
        start = time.time()
        try:
            response = requests.get(f"{self.base_url}{endpoint}", params=params, timeout=10)
            elapsed = (time.time() - start) * 1000  # æ¯«ç§’
            
            return {
                "endpoint": endpoint,
                "status_code": response.status_code,
                "elapsed_ms": elapsed,
                "success": response.status_code == 200
            }
        except Exception as e:
            return {
                "endpoint": endpoint,
                "error": str(e),
                "success": False
            }
    
    def check_query_count(self, endpoint: str, params: Dict = None) -> Dict:
        """æ£€æŸ¥æŸ¥è¯¢æ¬¡æ•°ï¼ˆé€šè¿‡å“åº”å¤´ï¼‰"""
        try:
            response = requests.get(f"{self.base_url}{endpoint}", params=params, timeout=10)
            query_count = int(response.headers.get("X-Query-Count", 0))
            
            return {
                "endpoint": endpoint,
                "query_count": query_count,
                "success": query_count <= 5  # æœŸæœ› <= 5 æ¬¡æŸ¥è¯¢
            }
        except Exception as e:
            return {
                "endpoint": endpoint,
                "error": str(e),
                "success": False
            }
    
    def check_cache_hit_rate(self) -> Dict:
        """æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡"""
        try:
            response = requests.get(f"{self.base_url}/admin/cache-stats", timeout=10)
            stats = response.json()
            
            hit_rate = float(stats.get("hit_rate", "0%").replace("%", ""))
            
            return {
                "hit_rate": hit_rate,
                "hits": stats.get("hits", 0),
                "misses": stats.get("misses", 0),
                "success": hit_rate >= 70  # æœŸæœ› >= 70%
            }
        except Exception as e:
            return {
                "error": str(e),
                "success": False
            }
    
    def run_all_checks(self):
        """è¿è¡Œæ‰€æœ‰æ£€æŸ¥"""
        print("ğŸ” å¼€å§‹ä¼˜åŒ–æ•ˆæœæ£€æŸ¥...\n")
        
        # 1. å“åº”æ—¶é—´æ£€æŸ¥
        print("1. æ£€æŸ¥å“åº”æ—¶é—´...")
        endpoints = [
            ("/tasks", {"limit": 20, "sort_by": "latest"}),
            ("/tasks", {"limit": 20, "task_type": "delivery", "sort_by": "latest"}),
            ("/my-applications", {}),
        ]
        
        for endpoint, params in endpoints:
            result = self.check_response_time(endpoint, params)
            self.results[f"response_time_{endpoint}"] = result
            status = "âœ…" if result.get("success") and result.get("elapsed_ms", 0) < 200 else "âŒ"
            print(f"  {status} {endpoint}: {result.get('elapsed_ms', 0):.2f}ms")
        
        # 2. æŸ¥è¯¢æ¬¡æ•°æ£€æŸ¥
        print("\n2. æ£€æŸ¥æŸ¥è¯¢æ¬¡æ•°...")
        for endpoint, params in endpoints:
            result = self.check_query_count(endpoint, params)
            self.results[f"query_count_{endpoint}"] = result
            status = "âœ…" if result.get("success") else "âŒ"
            print(f"  {status} {endpoint}: {result.get('query_count', 0)} æ¬¡æŸ¥è¯¢")
        
        # 3. ç¼“å­˜å‘½ä¸­ç‡æ£€æŸ¥
        print("\n3. æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡...")
        result = self.check_cache_hit_rate()
        self.results["cache_hit_rate"] = result
        status = "âœ…" if result.get("success") else "âŒ"
        print(f"  {status} ç¼“å­˜å‘½ä¸­ç‡: {result.get('hit_rate', 0):.2f}%")
        
        # 4. æ€»ç»“
        print("\nğŸ“Š æ£€æŸ¥æ€»ç»“:")
        total_checks = len(self.results)
        passed_checks = sum(1 for r in self.results.values() if r.get("success"))
        print(f"  é€šè¿‡: {passed_checks}/{total_checks}")
        
        if passed_checks == total_checks:
            print("  âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
        else:
            print("  âš ï¸  éƒ¨åˆ†æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
        
        return self.results

if __name__ == "__main__":
    checker = OptimizationChecker()
    results = checker.run_all_checks()
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    with open("optimization_check_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print("\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° optimization_check_results.json")
```

**è¿è¡Œæ£€æŸ¥è„šæœ¬**:

```bash
# å®‰è£…ä¾èµ–
pip install requests

# è¿è¡Œæ£€æŸ¥
python scripts/check_optimization.py
```

---

## 7. æŒç»­ç›‘æ§

### 7.1 è®¾ç½®æ€§èƒ½ç›‘æ§å‘Šè­¦

**ä½¿ç”¨ Prometheus + Grafana**:

```python
# backend/app/middleware/metrics.py
from prometheus_client import Counter, Histogram, Gauge
import time

# å®šä¹‰æŒ‡æ ‡
request_count = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
request_duration = Histogram('http_request_duration_seconds', 'HTTP request duration', ['method', 'endpoint'])
query_count = Histogram('db_queries_total', 'Database queries per request', ['endpoint'])
cache_hits = Counter('cache_hits_total', 'Cache hits', ['cache_type'])
cache_misses = Counter('cache_misses_total', 'Cache misses', ['cache_type'])

class MetricsMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        
        response = await call_next(request)
        
        # è®°å½•æŒ‡æ ‡
        elapsed = time.time() - start_time
        request_count.labels(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code
        ).inc()
        request_duration.labels(
            method=request.method,
            endpoint=request.url.path
        ).observe(elapsed)
        
        # ä»å“åº”å¤´è·å–æŸ¥è¯¢æ¬¡æ•°
        query_count_val = int(response.headers.get("X-Query-Count", 0))
        query_count.labels(endpoint=request.url.path).observe(query_count_val)
        
        return response
```

### 7.2 è®¾ç½®å‘Šè­¦è§„åˆ™

```yaml
# prometheus/alerts.yml
groups:
  - name: performance_alerts
    rules:
      - alert: HighResponseTime
        expr: http_request_duration_seconds{quantile="0.95"} > 0.2
        for: 5m
        annotations:
          summary: "å“åº”æ—¶é—´è¿‡é«˜"
          description: "{{ $labels.endpoint }} çš„ P95 å“åº”æ—¶é—´è¶…è¿‡ 200ms"
      
      - alert: TooManyQueries
        expr: db_queries_total{quantile="0.95"} > 10
        for: 5m
        annotations:
          summary: "æŸ¥è¯¢æ¬¡æ•°è¿‡å¤š"
          description: "{{ $labels.endpoint }} çš„æŸ¥è¯¢æ¬¡æ•°è¶…è¿‡ 10 æ¬¡"
      
      - alert: LowCacheHitRate
        expr: rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m])) < 0.7
        for: 10m
        annotations:
          summary: "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½"
          description: "ç¼“å­˜å‘½ä¸­ç‡ä½äº 70%"
```

---

## 8. æ£€æŸ¥æ¸…å•æ€»ç»“

### ä¼˜åŒ–å‰å‡†å¤‡

- [ ] è®°å½•ä¼˜åŒ–å‰çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆå“åº”æ—¶é—´ã€æŸ¥è¯¢æ¬¡æ•°ã€ç¼“å­˜å‘½ä¸­ç‡ï¼‰
- [ ] å¤‡ä»½æ•°æ®åº“
- [ ] å‡†å¤‡å›æ»šæ–¹æ¡ˆ

### ä¼˜åŒ–åæ£€æŸ¥

- [ ] **æ€§èƒ½æŒ‡æ ‡**
  - [ ] å“åº”æ—¶é—´é™ä½ 50% ä»¥ä¸Š
  - [ ] P95 å“åº”æ—¶é—´ < 200ms
  - [ ] æ•°æ®åº“æŸ¥è¯¢æ—¶é—´é™ä½ 50% ä»¥ä¸Š
  
- [ ] **N+1 æŸ¥è¯¢**
  - [ ] ç”¨æˆ·ç”³è¯·åˆ—è¡¨ï¼šæŸ¥è¯¢æ¬¡æ•° <= 3
  - [ ] ä»»åŠ¡ç”³è¯·åˆ—è¡¨ï¼šæŸ¥è¯¢æ¬¡æ•° <= 3
  - [ ] ä»»åŠ¡åˆ—è¡¨ï¼šæŸ¥è¯¢æ¬¡æ•° <= 2
  
- [ ] **ç´¢å¼•ä½¿ç”¨**
  - [ ] æ‰€æœ‰å…³é”®æŸ¥è¯¢éƒ½ä½¿ç”¨äº†ç´¢å¼•ï¼ˆEXPLAIN ANALYZEï¼‰
  - [ ] æ²¡æœ‰å‡ºç° Seq Scanï¼ˆå…¨è¡¨æ‰«æï¼‰
  - [ ] ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡æ˜¾ç¤ºæ–°ç´¢å¼•è¢«ä½¿ç”¨
  
- [ ] **ç¼“å­˜æ•ˆæœ**
  - [ ] ç¼“å­˜å‘½ä¸­ç‡ >= 70%
  - [ ] Redis å†…å­˜ä½¿ç”¨æ­£å¸¸
  - [ ] ç¼“å­˜å¤±æ•ˆé€»è¾‘æ­£ç¡®
  
- [ ] **åŠŸèƒ½æ­£ç¡®æ€§**
  - [ ] åˆ†é¡µæ•°æ®ä¸é‡å¤ã€ä¸é—æ¼
  - [ ] åˆ—è¡¨å’Œæ€»æ•°ä¸€è‡´
  - [ ] æœç´¢åŠŸèƒ½æ­£å¸¸

### æŒç»­ç›‘æ§

- [ ] è®¾ç½®æ€§èƒ½ç›‘æ§å‘Šè­¦
- [ ] å®šæœŸæ£€æŸ¥æ…¢æŸ¥è¯¢æ—¥å¿—
- [ ] å®šæœŸæ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
- [ ] å®šæœŸæ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡

---

## ğŸ“ æ£€æŸ¥æŠ¥å‘Šæ¨¡æ¿

```markdown
# ä¼˜åŒ–æ•ˆæœæ£€æŸ¥æŠ¥å‘Š

**æ£€æŸ¥æ—¥æœŸ**: 2025-01-27
**æ£€æŸ¥äººå‘˜**: [å§“å]

## 1. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| ä»»åŠ¡åˆ—è¡¨å¹³å‡å“åº”æ—¶é—´ | 300ms | 80ms | 3.75å€ |
| ä»»åŠ¡åˆ—è¡¨ P95 å“åº”æ—¶é—´ | 500ms | 150ms | 3.33å€ |
| ç”¨æˆ·ç”³è¯·åˆ—è¡¨æŸ¥è¯¢æ¬¡æ•° | 15æ¬¡ | 2æ¬¡ | 7.5å€ |

## 2. ç´¢å¼•ä½¿ç”¨æƒ…å†µ

- âœ… ix_tasks_status_created_id: å·²ä½¿ç”¨ï¼ŒæŸ¥è¯¢æ—¶é—´ < 10ms
- âœ… ix_tasks_type_location_status: å·²ä½¿ç”¨ï¼ŒæŸ¥è¯¢æ—¶é—´ < 15ms
- âœ… ix_messages_conversation_created: å·²ä½¿ç”¨ï¼ŒæŸ¥è¯¢æ—¶é—´ < 5ms

## 3. ç¼“å­˜æ•ˆæœ

- ç¼“å­˜å‘½ä¸­ç‡: 85%
- Redis å†…å­˜ä½¿ç”¨: 45MB
- ç¼“å­˜å¤±æ•ˆé€»è¾‘: æ­£å¸¸

## 4. é—®é¢˜å‘ç°

- [åˆ—å‡ºå‘ç°çš„é—®é¢˜ï¼Œå¦‚æœæœ‰]

## 5. å»ºè®®

- [åˆ—å‡ºåç»­ä¼˜åŒ–å»ºè®®]
```

---

**é‡è¦æç¤º**: 
- âš ï¸ **å¿…é¡»ä½¿ç”¨ EXPLAIN ANALYZE éªŒè¯ç´¢å¼•ä½¿ç”¨æƒ…å†µ**ï¼Œè¿™æ˜¯æœ€å¯é çš„æ£€æŸ¥æ–¹æ³•
- âš ï¸ **å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½æŒ‡æ ‡**ï¼Œç¡®ä¿ä¼˜åŒ–çœŸæ­£æœ‰æ•ˆ
- âš ï¸ **æ£€æŸ¥åŠŸèƒ½æ­£ç¡®æ€§**ï¼Œç¡®ä¿ä¼˜åŒ–æ²¡æœ‰å¼•å…¥ bug






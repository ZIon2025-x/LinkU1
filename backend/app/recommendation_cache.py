"""
æ¨èç³»ç»Ÿç¼“å­˜ä¼˜åŒ–æ¨¡å—
æä¾›æ›´é«˜æ•ˆçš„ç¼“å­˜ç­–ç•¥å’Œåºåˆ—åŒ–

æ³¨æ„ï¼šåªç¼“å­˜ä»»åŠ¡IDå’Œå…ƒæ•°æ®ï¼Œé¿å…åºåˆ—åŒ–SQLAlchemyå¯¹è±¡
"""

import json
import logging
import hashlib
from typing import List, Dict, Optional, Any
from datetime import datetime

from app.redis_cache import redis_cache

logger = logging.getLogger(__name__)


def _extract_cacheable_data(recommendations: List[Dict]) -> List[Dict]:
    """
    æå–å¯ç¼“å­˜çš„æ•°æ®ï¼ˆåªä¿ç•™IDå’Œå…ƒæ•°æ®ï¼Œä¸åŒ…å«SQLAlchemyå¯¹è±¡ï¼‰
    
    Args:
        recommendations: æ¨èç»“æœåˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å«Taskå¯¹è±¡ï¼‰
    
    Returns:
        å¯å®‰å…¨åºåˆ—åŒ–çš„æ•°æ®åˆ—è¡¨
    """
    cacheable = []
    for rec in recommendations:
        task = rec.get("task")
        if task:
            # æå–ä»»åŠ¡çš„å…³é”®ä¿¡æ¯ï¼Œé¿å…åºåˆ—åŒ–æ•´ä¸ªORMå¯¹è±¡
            cacheable.append({
                "task_id": task.id if hasattr(task, 'id') else task.get("task_id"),
                "score": rec.get("score", 0),
                "reason": rec.get("reason", ""),
                # ç¼“å­˜ä¸€äº›å¸¸ç”¨å­—æ®µï¼Œå‡å°‘åç»­æŸ¥è¯¢
                "title": getattr(task, 'title', None) or task.get("title"),
                "task_type": getattr(task, 'task_type', None) or task.get("task_type"),
                "location": getattr(task, 'location', None) or task.get("location"),
            })
        else:
            # å¦‚æœæ²¡æœ‰taskå¯¹è±¡ï¼Œå¯èƒ½å·²ç»æ˜¯å¤„ç†è¿‡çš„æ•°æ®
            cacheable.append(rec)
    return cacheable


def serialize_recommendations(recommendations: List[Dict]) -> str:
    """
    åºåˆ—åŒ–æ¨èç»“æœï¼ˆä½¿ç”¨JSONï¼Œåªåºåˆ—åŒ–IDå’Œå…ƒæ•°æ®ï¼‰
    
    Args:
        recommendations: æ¨èç»“æœåˆ—è¡¨
    
    Returns:
        åºåˆ—åŒ–åçš„JSONå­—ç¬¦ä¸²
    """
    try:
        # æå–å¯ç¼“å­˜æ•°æ®ï¼ˆä¸åŒ…å«SQLAlchemyå¯¹è±¡ï¼‰
        cacheable_data = _extract_cacheable_data(recommendations)
        return json.dumps(cacheable_data, ensure_ascii=False, default=str)
    except Exception as e:
        logger.warning(f"åºåˆ—åŒ–æ¨èç»“æœå¤±è´¥: {e}")
        return "[]"


def deserialize_recommendations(data) -> List[Dict]:
    """
    ååºåˆ—åŒ–æ¨èç»“æœ
    
    æ³¨æ„ï¼šè¿”å›çš„æ•°æ®åªåŒ…å«ä»»åŠ¡IDå’Œå…ƒæ•°æ®ï¼Œéœ€è¦è°ƒç”¨æ–¹æ ¹æ®IDæŸ¥è¯¢å®Œæ•´ä»»åŠ¡ä¿¡æ¯
    
    Args:
        data: åºåˆ—åŒ–åçš„æ•°æ®
    
    Returns:
        æ¨èç»“æœåˆ—è¡¨ï¼ˆä¸åŒ…å«å®Œæ•´Taskå¯¹è±¡ï¼‰
    """
    # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
    if isinstance(data, list):
        return data
    
    # å¦‚æœæ˜¯å­—å…¸ï¼Œå¯èƒ½æ˜¯å•ä¸ªç»“æœï¼ŒåŒ…è£…æˆåˆ—è¡¨
    if isinstance(data, dict):
        return [data]
    
    # å¦‚æœæ˜¯ bytesï¼Œè§£ç åè§£æ
    if isinstance(data, bytes):
        try:
            return json.loads(data.decode('utf-8'))
        except Exception as e:
            logger.error(f"ååºåˆ—åŒ–æ¨èç»“æœå¤±è´¥: {e}")
            return []
    
    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯• JSON è§£æ
    if isinstance(data, str):
        try:
            result = json.loads(data)
            if isinstance(result, list):
                return result
            return [result] if result else []
        except Exception as e:
            logger.error(f"ååºåˆ—åŒ–æ¨èç»“æœå¤±è´¥: {e}")
            return []
    
    logger.warning(f"æœªçŸ¥çš„æ•°æ®ç±»å‹: {type(data)}")
    return []


def get_cache_key(
    user_id: str,
    algorithm: str,
    limit: int,
    task_type: Optional[str] = None,
    location: Optional[str] = None,
    keyword: Optional[str] = None
) -> str:
    """
    ç”Ÿæˆç¼“å­˜é”®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨å“ˆå¸Œç¼©çŸ­é”®é•¿åº¦ï¼‰
    
    Args:
        user_id: ç”¨æˆ·ID
        algorithm: æ¨èç®—æ³•
        limit: æ¨èæ•°é‡
        task_type: ä»»åŠ¡ç±»å‹
        location: åœ°ç‚¹
        keyword: å…³é”®è¯
    
    Returns:
        ç¼“å­˜é”®
    """
    # æ„å»ºé”®çš„ç»„æˆéƒ¨åˆ†
    parts = [
        "rec",
        user_id,
        algorithm,
        str(limit),
        task_type or "all",
        location or "all",
        keyword or "all"
    ]
    
    # å¦‚æœé”®å¤ªé•¿ï¼Œä½¿ç”¨å“ˆå¸Œç¼©çŸ­
    key = ":".join(parts)
    if len(key) > 200:  # Redisé”®é•¿åº¦é™åˆ¶
        # ğŸ”’ å®‰å…¨ä¿®å¤ï¼šä½¿ç”¨SHA256æ›¿ä»£MD5[:8]ï¼Œå‡å°‘ç¢°æ’é£é™©
        # MD5[:8]åªæœ‰2^32ç©ºé—´(~65Kæ¡ç›®50%ç¢°æ’)ï¼ŒSHA256[:16]æœ‰2^64ç©ºé—´
        filter_part = ":".join(parts[4:])
        filter_hash = hashlib.sha256(filter_part.encode()).hexdigest()[:16]
        key = ":".join(parts[:4] + [filter_hash])
    
    return key


def cache_recommendations(
    cache_key: str,
    recommendations: List[Dict],
    ttl: int = 1800  # 30åˆ†é’Ÿ
) -> bool:
    """
    ç¼“å­˜æ¨èç»“æœï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        cache_key: ç¼“å­˜é”®
        recommendations: æ¨èç»“æœ
        ttl: ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        serialized = serialize_recommendations(recommendations)
        redis_cache.setex(cache_key, ttl, serialized)
        return True
    except Exception as e:
        logger.warning(f"ç¼“å­˜æ¨èç»“æœå¤±è´¥: {e}")
        return False


def get_cached_recommendations(cache_key: str) -> Optional[List[Dict]]:
    """
    è·å–ç¼“å­˜çš„æ¨èç»“æœï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        cache_key: ç¼“å­˜é”®
    
    Returns:
        æ¨èç»“æœåˆ—è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
    """
    try:
        cached = redis_cache.get(cache_key)
        if cached:
            return deserialize_recommendations(cached)
    except Exception as e:
        logger.warning(f"è·å–ç¼“å­˜æ¨èç»“æœå¤±è´¥: {e}")
    
    return None


def invalidate_user_recommendations(user_id: str):
    """
    æ¸…é™¤ç”¨æˆ·çš„æ‰€æœ‰æ¨èç¼“å­˜
    
    å½“ç”¨æˆ·è¡Œä¸ºå‘ç”Ÿå˜åŒ–æ—¶è°ƒç”¨
    
    Args:
        user_id: ç”¨æˆ·ID
    """
    try:
        # ä½¿ç”¨ scan ä»£æ›¿ keys å‘½ä»¤ï¼Œé¿å…é˜»å¡ Redis
        pattern = f"rec:{user_id}:*"
        deleted_count = 0
        
        # ä½¿ç”¨ scan_iter è¿­ä»£åŒ¹é…çš„é”®
        cursor = 0
        while True:
            cursor, keys = redis_cache.scan(cursor, match=pattern, count=100)
            if keys:
                redis_cache.delete(*keys)
                deleted_count += len(keys)
            if cursor == 0:
                break
        
        if deleted_count > 0:
            logger.info(f"æ¸…é™¤ç”¨æˆ·æ¨èç¼“å­˜: user_id={user_id}, count={deleted_count}")
    except AttributeError:
        # é™çº§ï¼šä½¿ç”¨ redis_utils çš„ SCAN å®ç°
        try:
            from app.redis_utils import delete_by_pattern
            deleted = delete_by_pattern(redis_cache, pattern)
            if deleted > 0:
                logger.info(f"æ¸…é™¤ç”¨æˆ·æ¨èç¼“å­˜(scan): user_id={user_id}, count={deleted}")
        except Exception as e:
            logger.warning(f"æ¸…é™¤ç”¨æˆ·æ¨èç¼“å­˜å¤±è´¥: {e}")
    except Exception as e:
        logger.warning(f"æ¸…é™¤ç”¨æˆ·æ¨èç¼“å­˜å¤±è´¥: {e}")


def get_cache_stats() -> Dict[str, Any]:
    """
    è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨ scan é¿å…é˜»å¡ï¼‰
    
    Returns:
        ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        # ä½¿ç”¨ scan ä»£æ›¿ keys å‘½ä»¤
        pattern = "rec:*"
        total_keys = 0
        sample_keys = []
        
        cursor = 0
        while True:
            cursor, keys = redis_cache.scan(cursor, match=pattern, count=100)
            total_keys += len(keys)
            if len(sample_keys) < 10:
                sample_keys.extend(keys[:10 - len(sample_keys)])
            if cursor == 0:
                break
        
        stats = {
            "total_keys": total_keys,
            "sample_keys": sample_keys
        }
        
        return stats
    except AttributeError:
        # é™çº§ï¼šä½¿ç”¨ redis_utils çš„ SCAN å®ç°
        try:
            from app.redis_utils import scan_keys
            keys = scan_keys(redis_cache, pattern)
            return {
                "total_keys": len(keys) if keys else 0,
                "sample_keys": (keys[:10] if keys else [])
            }
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}
    except Exception as e:
        logger.warning(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
        return {"error": str(e)}
